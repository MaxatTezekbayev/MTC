{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    }
   ],
   "source": [
    "#train CAE+H (1001 MB)\n",
    "CUDA_VISIBLE_DEVICES=1 python main.py --train_CAEH True --epochs 20 --lambd 1e-05 --gamma 1e-07 --numlayers 2 --code_size 120  --code_size2 60   --save_dir_for_CAE saved_weights/120_60_1e-05_1e-07.pth \n",
    "\n",
    "\n",
    "\n",
    "#\n",
    "CUDA_VISIBLE_DEVICES=1 python main.py --train_CAEH False --MTC True  --lambd 1e-05 --gamma 1e-07 --numlayers 2 --code_size 120 --code_size2 60   --pretrained_CAEH saved_weights/120_60_1e-05_1e-07.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "float"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start_time_model = time.time()\n",
    "time.sleep(1)\n",
    "type(time.time()-start_time_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CUDA_VISIBLE_DEVICES=1 python main.py --train_CAEH True --CAEH True --epochs 20 --lambd 1e-05 --gamma 1e-07 --numlayers 2 --code_size 392  --code_size2 60   --KNN True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA_VISIBLE_DEVICES=5 python main.py --train_CAEH True   --epochs 60 --lambd 1e-07 --gamma 1e-07 --numlayers 2 --code_size 392 --code_size2 196 --save_dir_for_CAE saved_weights/392_196_1e-07_1e-07.pth  & \n",
      "CUDA_VISIBLE_DEVICES=5 python main.py --train_CAEH True   --epochs 60 --lambd 1e-07 --gamma 1e-06 --numlayers 2 --code_size 392 --code_size2 196 --save_dir_for_CAE saved_weights/392_196_1e-07_1e-06.pth  & \n",
      "CUDA_VISIBLE_DEVICES=5 python main.py --train_CAEH True   --epochs 60 --lambd 1e-07 --gamma 1e-05 --numlayers 2 --code_size 392 --code_size2 196 --save_dir_for_CAE saved_weights/392_196_1e-07_1e-05.pth  & \n",
      "CUDA_VISIBLE_DEVICES=5 python main.py --train_CAEH True   --epochs 60 --lambd 1e-07 --gamma 0.0001 --numlayers 2 --code_size 392 --code_size2 196 --save_dir_for_CAE saved_weights/392_196_1e-07_0.0001.pth  & \n",
      "CUDA_VISIBLE_DEVICES=5 python main.py --train_CAEH True   --epochs 60 --lambd 1e-07 --gamma 0 --numlayers 2 --code_size 392 --code_size2 196 --save_dir_for_CAE saved_weights/392_196_1e-07_0.pth  & \n",
      "CUDA_VISIBLE_DEVICES=5 python main.py --train_CAEH True   --epochs 60 --lambd 1e-06 --gamma 1e-07 --numlayers 2 --code_size 392 --code_size2 196 --save_dir_for_CAE saved_weights/392_196_1e-06_1e-07.pth  & \n",
      "CUDA_VISIBLE_DEVICES=5 python main.py --train_CAEH True   --epochs 60 --lambd 1e-06 --gamma 1e-06 --numlayers 2 --code_size 392 --code_size2 196 --save_dir_for_CAE saved_weights/392_196_1e-06_1e-06.pth  & \n",
      "CUDA_VISIBLE_DEVICES=5 python main.py --train_CAEH True   --epochs 60 --lambd 1e-06 --gamma 1e-05 --numlayers 2 --code_size 392 --code_size2 196 --save_dir_for_CAE saved_weights/392_196_1e-06_1e-05.pth  & \n",
      "CUDA_VISIBLE_DEVICES=5 python main.py --train_CAEH True   --epochs 60 --lambd 1e-06 --gamma 0.0001 --numlayers 2 --code_size 392 --code_size2 196 --save_dir_for_CAE saved_weights/392_196_1e-06_0.0001.pth  & \n",
      "CUDA_VISIBLE_DEVICES=5 python main.py --train_CAEH True   --epochs 60 --lambd 1e-06 --gamma 0 --numlayers 2 --code_size 392 --code_size2 196 --save_dir_for_CAE saved_weights/392_196_1e-06_0.pth  & \n",
      "CUDA_VISIBLE_DEVICES=5 python main.py --train_CAEH True   --epochs 60 --lambd 1e-05 --gamma 1e-07 --numlayers 2 --code_size 392 --code_size2 196 --save_dir_for_CAE saved_weights/392_196_1e-05_1e-07.pth  & \n",
      "CUDA_VISIBLE_DEVICES=5 python main.py --train_CAEH True   --epochs 60 --lambd 1e-05 --gamma 1e-06 --numlayers 2 --code_size 392 --code_size2 196 --save_dir_for_CAE saved_weights/392_196_1e-05_1e-06.pth  & \n",
      "CUDA_VISIBLE_DEVICES=5 python main.py --train_CAEH True   --epochs 60 --lambd 1e-05 --gamma 1e-05 --numlayers 2 --code_size 392 --code_size2 196 --save_dir_for_CAE saved_weights/392_196_1e-05_1e-05.pth  & \n",
      "CUDA_VISIBLE_DEVICES=5 python main.py --train_CAEH True   --epochs 60 --lambd 1e-05 --gamma 0.0001 --numlayers 2 --code_size 392 --code_size2 196 --save_dir_for_CAE saved_weights/392_196_1e-05_0.0001.pth  & \n",
      "CUDA_VISIBLE_DEVICES=6 python main.py --train_CAEH True   --epochs 60 --lambd 1e-05 --gamma 0 --numlayers 2 --code_size 392 --code_size2 196 --save_dir_for_CAE saved_weights/392_196_1e-05_0.pth  & \n",
      "CUDA_VISIBLE_DEVICES=6 python main.py --train_CAEH True   --epochs 60 --lambd 0.0001 --gamma 1e-07 --numlayers 2 --code_size 392 --code_size2 196 --save_dir_for_CAE saved_weights/392_196_0.0001_1e-07.pth  & \n",
      "CUDA_VISIBLE_DEVICES=6 python main.py --train_CAEH True   --epochs 60 --lambd 0.0001 --gamma 1e-06 --numlayers 2 --code_size 392 --code_size2 196 --save_dir_for_CAE saved_weights/392_196_0.0001_1e-06.pth  & \n",
      "CUDA_VISIBLE_DEVICES=6 python main.py --train_CAEH True   --epochs 60 --lambd 0.0001 --gamma 1e-05 --numlayers 2 --code_size 392 --code_size2 196 --save_dir_for_CAE saved_weights/392_196_0.0001_1e-05.pth  & \n",
      "CUDA_VISIBLE_DEVICES=6 python main.py --train_CAEH True   --epochs 60 --lambd 0.0001 --gamma 0.0001 --numlayers 2 --code_size 392 --code_size2 196 --save_dir_for_CAE saved_weights/392_196_0.0001_0.0001.pth  & \n",
      "CUDA_VISIBLE_DEVICES=6 python main.py --train_CAEH True   --epochs 60 --lambd 0.0001 --gamma 0 --numlayers 2 --code_size 392 --code_size2 196 --save_dir_for_CAE saved_weights/392_196_0.0001_0.pth  & \n",
      "CUDA_VISIBLE_DEVICES=6 python main.py --train_CAEH True   --epochs 60 --lambd 0 --gamma 1e-07 --numlayers 2 --code_size 392 --code_size2 196 --save_dir_for_CAE saved_weights/392_196_0_1e-07.pth  & \n",
      "CUDA_VISIBLE_DEVICES=6 python main.py --train_CAEH True   --epochs 60 --lambd 0 --gamma 1e-06 --numlayers 2 --code_size 392 --code_size2 196 --save_dir_for_CAE saved_weights/392_196_0_1e-06.pth  & \n",
      "CUDA_VISIBLE_DEVICES=6 python main.py --train_CAEH True   --epochs 60 --lambd 0 --gamma 1e-05 --numlayers 2 --code_size 392 --code_size2 196 --save_dir_for_CAE saved_weights/392_196_0_1e-05.pth  & \n",
      "CUDA_VISIBLE_DEVICES=6 python main.py --train_CAEH True   --epochs 60 --lambd 0 --gamma 0.0001 --numlayers 2 --code_size 392 --code_size2 196 --save_dir_for_CAE saved_weights/392_196_0_0.0001.pth  & \n",
      "CUDA_VISIBLE_DEVICES=6 python main.py --train_CAEH True   --epochs 60 --lambd 0 --gamma 0 --numlayers 2 --code_size 392 --code_size2 196 --save_dir_for_CAE saved_weights/392_196_0_0.pth  & \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "command='CUDA_VISIBLE_DEVICES={0} python main.py --train_CAEH True   --epochs 60 --lambd {1} --gamma {2} --numlayers 2 --code_size {3} --code_size2 {4} --save_dir_for_CAE saved_weights/{3}_{4}_{1}_{2}.pth  '\n",
    "\n",
    "\n",
    "\n",
    "code_sizes=[392]\n",
    "code_sizes2=[196]\n",
    "\n",
    "lambds=[0.0000001, 0.000001, 0.00001, 0.0001, 0]\n",
    "gammas=[0.0000001, 0.000001, 0.00001, 0.0001, 0]\n",
    "\n",
    "\n",
    "per_gpu=15\n",
    "gpus=[5, 6]\n",
    "i=1\n",
    "for code_size in code_sizes:\n",
    "    for code_size2 in code_sizes2:\n",
    "        for lambd in lambds:\n",
    "            for gamma in gammas:\n",
    "                end=\"& \\n\"\n",
    "                if i%((per_gpu*len(gpus)))==0:\n",
    "                    end=' \\n \\n'\n",
    "\n",
    "                print(command.format(gpus[(i%(per_gpu*len(gpus)))//per_gpu], lambd, gamma, code_size, code_size2),end=end)\n",
    "                i += 1\n",
    "i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA_VISIBLE_DEVICES=3 python main.py --pretrained_CAEH saved_weights/392_196_1e-07_1e-07.pth --MTC True  --lambd 1e-07 --gamma 1e-07 --numlayers 2 --code_size 392 --code_size2 196 & \n",
      "CUDA_VISIBLE_DEVICES=3 python main.py --pretrained_CAEH saved_weights/392_196_1e-07_0.pth --MTC True  --lambd 1e-07 --gamma 0 --numlayers 2 --code_size 392 --code_size2 196 & \n",
      "CUDA_VISIBLE_DEVICES=3 python main.py --pretrained_CAEH saved_weights/392_196_0_1e-07.pth --MTC True  --lambd 0 --gamma 1e-07 --numlayers 2 --code_size 392 --code_size2 196 & \n",
      "CUDA_VISIBLE_DEVICES=3 python main.py --pretrained_CAEH saved_weights/392_196_0_0.pth --MTC True  --lambd 0 --gamma 0 --numlayers 2 --code_size 392 --code_size2 196 & \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "command='CUDA_VISIBLE_DEVICES={0} python main.py --pretrained_CAEH saved_weights/{3}_{4}_{1}_{2}.pth --MTC True  --lambd {1} --gamma {2} --numlayers 2 --code_size {3} --code_size2 {4} '\n",
    "\n",
    "\n",
    "\n",
    "code_sizes=[392]\n",
    "code_sizes2=[196]\n",
    "\n",
    "lambds=[0.0000001, 0]\n",
    "gammas=[0.0000001, 0]\n",
    "\n",
    "\n",
    "per_gpu=13\n",
    "gpus=[3, 5, 7]\n",
    "i=1\n",
    "for code_size in code_sizes:\n",
    "    for code_size2 in code_sizes2:\n",
    "        for lambd in lambds:\n",
    "            for gamma in gammas:\n",
    "                end=\"& \\n\"\n",
    "                if i%((per_gpu*len(gpus)))==0:\n",
    "                    end=' \\n \\n'\n",
    "\n",
    "                print(command.format(gpus[(i%(per_gpu*len(gpus)))//per_gpu], lambd, gamma, code_size, code_size2),end=end)\n",
    "                i += 1\n",
    "i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA_VISIBLE_DEVICES=3 python main.py --train_CAEH True   --epochs 80 --lambd 1e-07 --gamma 1e-07 --batch_size 150 --numlayers 2 --code_size 2000 --code_size2 2000 --save_dir_for_CAE saved_weights/2000_2000_1e-07_1e-07.pth  & \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### 2000 2000\n",
    "\n",
    "\n",
    "command='CUDA_VISIBLE_DEVICES={0} python main.py --train_CAEH True   --epochs 80 --lambd {1} --gamma {2} --batch_size 150 --numlayers 2 --code_size {3} --code_size2 {4} --save_dir_for_CAE saved_weights/{3}_{4}_{1}_{2}.pth  '\n",
    "\n",
    "\n",
    "\n",
    "code_sizes=[2000]\n",
    "code_sizes2=[2000]\n",
    "\n",
    "lambds=[0.0000001]\n",
    "gammas=[0.0000001]\n",
    "\n",
    "\n",
    "per_gpu=15\n",
    "gpus=[3, 6]\n",
    "i=1\n",
    "for code_size in code_sizes:\n",
    "    for code_size2 in code_sizes2:\n",
    "        for lambd in lambds:\n",
    "            for gamma in gammas:\n",
    "                end=\"& \\n\"\n",
    "                if i%((per_gpu*len(gpus)))==0:\n",
    "                    end=' \\n \\n'\n",
    "\n",
    "                print(command.format(gpus[(i%(per_gpu*len(gpus)))//per_gpu], lambd, gamma, code_size, code_size2),end=end)\n",
    "                i += 1\n",
    "i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA_VISIBLE_DEVICES=2 python main.py --ALTER True --batch_size 20 --M 60 --epochs 40 --lambd 10.0 --gamma 1.0 --numlayers 2 --code_size 120 --code_size2 60 --save_dir_for_ALTER saved_weights/alter_120_60_10.0_1.0.pth & \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "command='CUDA_VISIBLE_DEVICES={0} python main.py --ALTER True --batch_size 20 --M 60 --epochs 40 --lambd {1} --gamma {2} --numlayers 2 --code_size {3} --code_size2 {4} --save_dir_for_ALTER saved_weights/alter_{3}_{4}_{1}_{2}.pth '\n",
    "\n",
    "\n",
    "\n",
    "code_sizes=[120]\n",
    "code_sizes2=[60]\n",
    "\n",
    "lambds=[10.0]\n",
    "gammas=[1.0]\n",
    "\n",
    "\n",
    "per_gpu=15\n",
    "gpus=[2]\n",
    "i=1\n",
    "for code_size in code_sizes:\n",
    "    for code_size2 in code_sizes2:\n",
    "        for lambd in lambds:\n",
    "            for gamma in gammas:\n",
    "                end=\"& \\n\"\n",
    "                if i%((per_gpu*len(gpus)))==0:\n",
    "                    end=' \\n \\n'\n",
    "\n",
    "                print(command.format(gpus[(i%(per_gpu*len(gpus)))//per_gpu], lambd, gamma, code_size, code_size2),end=end)\n",
    "                i += 1\n",
    "i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dsf\n"
     ]
    }
   ],
   "source": [
    "a=1\n",
    "b=2\n",
    "if (a is not None) and (b is not None):\n",
    "    print('dsf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.6.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import SubsetRandomSampler\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from models import  CAE2Layer, MTC,  ALTER2Layer\n",
    "from utils import cae_h_loss, MTC_loss, calculate_singular_vectors_B, knn_distances, sigmoid\n",
    "import argparse\n",
    "from collections import Counter\n",
    "torch.manual_seed(42)\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "\n",
    "code_size = 196\n",
    "code_size2 = 60\n",
    "\n",
    "batch_size=20\n",
    "image_size = 28\n",
    "dimensionality = image_size*image_size\n",
    "train_dataset = datasets.MNIST('data', train=True, download=True, transform=transforms.ToTensor())\n",
    "test_dataset = datasets.MNIST('data', train=False, download=True, transform=transforms.ToTensor())\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "M = len(train_dataset)//100\n",
    "k=30\n",
    "indices = torch.randperm(len(train_dataset))[:M]\n",
    "train_z_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, sampler=SubsetRandomSampler(indices))\n",
    "    \n",
    "model = ALTER2Layer(dimensionality, [code_size, code_size2])\n",
    "model.cuda()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "# model.load_state_dict(torch.load('/home/maxat/Projects/MTC/saved_weights/196_60_1e-05_1e-07.pth'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([20, 1, 28, 28]), tensor([0.]))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B = torch.zeros((len(train_z_loader),1))\n",
    "z_b_iter = iter(zip(train_z_loader,B))\n",
    "(z,_), b = next(z_b_iter)\n",
    "z.shape, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([60])\n"
     ]
    }
   ],
   "source": [
    "for step, (imgs, _) in enumerate(test_loader):\n",
    "    imgs = imgs.view(batch_size, -1).cuda()\n",
    "    imgs.requires_grad_(True)\n",
    "    recover, code_data, Jac = model(imgs, calculate_jacobian = True)\n",
    "    sigma_prime1 = torch.mul(1.0 - code_data[0], code_data[0])\n",
    "    print(sigma_prime1.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "grad_output=torch.ones(batch_size).cuda()\n",
    "Jac_auto=[]   \n",
    "for i in range(recover.shape[1]):\n",
    "    Jac_auto.append(torch.autograd.grad(outputs=recover[:,i], inputs=imgs, grad_outputs=grad_output, retain_graph=True, create_graph=True)[0])\n",
    "Jac_auto=torch.reshape(torch.cat(Jac_auto,1),[imgs.shape[0], recover.shape[1], imgs.shape[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "U_batch_auto, S_batch_auto, V_batch_auto = torch.svd(Jac_auto.cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "B_auto = torch.matmul(U_batch_auto[:, :, :k], torch.matmul(torch.diag_embed(S_batch_auto)[:, :k, :k], torch.transpose(V_batch_auto[:, :, :k],1,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "U_batch, S_batch, V_batch = torch.svd(Jac.cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.matmul(u[0], np.matmul(np.diag(s[0]),v[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "U_batch2, S_batch2, V_batch2 = torch.svd(Jac.cuda())\n",
    "U_batch2= U_batch2.cpu()\n",
    "S_batch2=S_batch2.cpu()\n",
    "V_batch2=V_batch2.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "B1 = torch.matmul(U_batch[:, :, :k], torch.matmul(torch.diag_embed(S_batch)[:, :k, :k], torch.transpose(V_batch[:, :, :k],1,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "B2 = torch.matmul(U_batch2[:, :, :k], torch.matmul(torch.diag_embed(S_batch2)[:, :k, :k], torch.transpose(V_batch2[:, :, :k],1,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.2689, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(B1-B_auto).abs().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.3453, device='cuda:0', grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.3816e-06, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(B2-B1).abs().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "u, s, v = np.linalg.svd(Jac.cpu().detach().numpy(), full_matrices = False)\n",
    "u = torch.from_numpy(u)\n",
    "s = torch.from_numpy(s)\n",
    "v = torch.from_numpy(v)\n",
    "B3_np = torch.matmul(u[:, :, :k], torch.matmul(torch.diag_embed(s)[:, :k, :k], v[:, :k, :]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0641e-06, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(B2-B3_np).abs().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2.4302e+02, 2.2956e+02, 2.1946e+02,  ..., 3.1773e-08, 2.4306e-08,\n",
       "         1.5292e-08],\n",
       "        [2.5005e+02, 2.2884e+02, 2.2027e+02,  ..., 3.6040e-08, 1.4925e-08,\n",
       "         1.2360e-08],\n",
       "        [2.5054e+02, 2.3045e+02, 2.2801e+02,  ..., 3.5430e-08, 8.4788e-09,\n",
       "         3.8526e-10],\n",
       "        ...,\n",
       "        [2.5552e+02, 2.3798e+02, 2.1724e+02,  ..., 4.8235e-08, 2.5371e-08,\n",
       "         1.1783e-08],\n",
       "        [2.4811e+02, 2.3084e+02, 2.2098e+02,  ..., 3.2949e-08, 2.9907e-08,\n",
       "         1.2612e-09],\n",
       "        [2.5767e+02, 2.4244e+02, 2.2125e+02,  ..., 2.0308e-08, 1.7075e-08,\n",
       "         6.0816e-09]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2.4302e+02, 2.2956e+02, 2.1946e+02,  ..., 1.3994e-06, 9.0363e-07,\n",
       "         3.0358e-07],\n",
       "        [2.5005e+02, 2.2884e+02, 2.2027e+02,  ..., 1.3951e-06, 8.3259e-07,\n",
       "         2.7739e-07],\n",
       "        [2.5054e+02, 2.3045e+02, 2.2801e+02,  ..., 1.5312e-06, 1.0099e-06,\n",
       "         3.3516e-07],\n",
       "        ...,\n",
       "        [2.5552e+02, 2.3798e+02, 2.1724e+02,  ..., 1.6037e-06, 9.6142e-07,\n",
       "         3.5925e-07],\n",
       "        [2.4811e+02, 2.3084e+02, 2.2098e+02,  ..., 1.5524e-06, 9.2001e-07,\n",
       "         3.4789e-07],\n",
       "        [2.5767e+02, 2.4244e+02, 2.2125e+02,  ..., 1.6376e-06, 1.0138e-06,\n",
       "         3.8954e-07]], grad_fn=<SvdBackward>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "S_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_data1 = model.sigmoid(torch.matmul(imgs, model.W1.t()) + model.b1)\n",
    "code_data2 = model.sigmoid(torch.matmul(code_data1, model.W2.t()) + model.b2)\n",
    "#decode\n",
    "code_data3 = model.sigmoid(torch.matmul(code_data2, model.W3.t()) + model.b3)\n",
    "recover = torch.matmul(code_data3, model.W4.t()) + model.b_r\n",
    "\n",
    "Jac4 = []\n",
    "for i in range(imgs.shape[0]): \n",
    "    diag_sigma_prime1 = torch.diag( torch.mul(1.0 - code_data1[i], code_data1[i]))\n",
    "    grad_1 = torch.matmul(diag_sigma_prime1, model.W1)\n",
    "\n",
    "    diag_sigma_prime2 = torch.diag( torch.mul(1.0 - code_data2[i], code_data2[i]))\n",
    "    grad_2 = torch.matmul(diag_sigma_prime2, model.W2)\n",
    "\n",
    "    diag_sigma_prime3  = torch.diag( torch.mul(1.0 - code_data3[i], code_data3[i]))\n",
    "    grad_3 = torch.matmul(diag_sigma_prime3,model.W3)\n",
    "    grad_4 = model.W4\n",
    "    Jac4.append(torch.matmul(grad_4, torch.matmul(grad_3, torch.matmul(grad_2, grad_1))))\n",
    "# Jac4 = torch.reshape(torch.cat(Jac4,1), [imgs.shape[0], recover.shape[1], imgs.shape[1]])\n",
    "Jac4=torch.stack(Jac4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.0299e-07, device='cuda:0', grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(Jac4-Jac_auto).abs().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.2689, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "U_batch4, S_batch4, V_batch4 = torch.svd(Jac4.cpu())\n",
    "B4 = torch.matmul(U_batch4[:, :, :k], torch.matmul(torch.diag_embed(S_batch4)[:, :k, :k], torch.transpose(V_batch4[:, :, :k],1,2)))\n",
    "(B2-B4).abs().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.09857559204101562\n"
     ]
    }
   ],
   "source": [
    "code_data1 = model.sigmoid(torch.matmul(imgs, model.W1.t()) + model.b1)\n",
    "code_data2 = model.sigmoid(torch.matmul(code_data1, model.W2.t()) + model.b2)\n",
    "#decode\n",
    "code_data3 = model.sigmoid(torch.matmul(code_data2, model.W3.t()) + model.b3)\n",
    "recover = torch.matmul(code_data3, model.W4.t()) + model.b_r\n",
    "\n",
    "\n",
    "A_matrix = []\n",
    "B_matrix = []\n",
    "C_matrix = []\n",
    "for i in range(imgs.shape[0]): \n",
    "    diag_sigma_prime1 = torch.diag( torch.mul(1.0 - code_data1[i], code_data1[i]))\n",
    "    grad_1 = torch.matmul(diag_sigma_prime1, model.W1)\n",
    "\n",
    "    diag_sigma_prime2 = torch.diag( torch.mul(1.0 - code_data2[i], code_data2[i]))\n",
    "    grad_2 = torch.matmul(diag_sigma_prime2, model.W2)\n",
    "\n",
    "    diag_sigma_prime3  = torch.diag( torch.mul(1.0 - code_data3[i], code_data3[i]))\n",
    "    grad_3 = torch.matmul(diag_sigma_prime3,model.W3)\n",
    "\n",
    "    A_matrix.append(grad_1)\n",
    "    B_matrix.append(grad_2)\n",
    "    C_matrix.append(grad_3)\n",
    "    \n",
    "A_matrix = torch.stack(A_matrix)\n",
    "B_matrix = torch.stack(B_matrix)\n",
    "C_matrix = torch.stack(C_matrix)\n",
    "\n",
    "def svd_product(A, U, S, VH): # A*U*S*VH\n",
    "    Q, R = torch.qr(torch.matmul(A.cuda(), U.cuda()).cpu())\n",
    "    u_temp, s_temp, vh_temp = torch.svd(torch.matmul(R.cuda(), torch.diag(S.cuda())).cpu())\n",
    "    return [torch.matmul(Q.cuda(), u_temp.cuda()), s_temp.cuda(), torch.matmul(vh_temp.T.cuda(),VH.cuda())]\n",
    "\n",
    "def svd_drei(A, B, C, D): # A*B*C*D\n",
    "    U_temp, S_temp, VH_temp = torch.svd(torch.matmul(C, D).cpu())\n",
    "    return svd_product(torch.matmul(A, B), U_temp.cuda(), S_temp.cuda(), VH_temp.T.cuda())\n",
    "\n",
    "import time\n",
    "start = time.time()\n",
    "B5_np=[]\n",
    "U=[]\n",
    "S=[]\n",
    "VH=[]\n",
    "W4 = model.W4.clone()\n",
    "for i in range(len(A_matrix)):\n",
    "#     u, s, vh = svd_drei(A_matrix[i].cpu(), B_matrix[i].cpu(), C_matrix[i].cpu(), U_W4, S_W4, VH_W4.T)\n",
    "    u, s, vh = svd_drei(W4, C_matrix[i], B_matrix[i],  A_matrix[i])\n",
    "    U.append(u)\n",
    "    S.append(s)\n",
    "    VH.append(vh)\n",
    "U = torch.stack(U)\n",
    "S = torch.stack(S)\n",
    "VH = torch.stack(VH)\n",
    "B5 = torch.matmul(U[:, :, :k], torch.matmul(torch.diag_embed(S)[:, :k, :k], VH[:, :k, :]))\n",
    "print(time.time()-start)\n",
    "# B5_np = torch.stack(B5_np)\n",
    "\n",
    "# B5=[]\n",
    "# U_W4, S_W4, VH_W4 = torch.svd(model.W1.clone().cpu())\n",
    "\n",
    "# U_ = []\n",
    "# S_ = []\n",
    "# V_ = []\n",
    "# for i in range(len(A_matrix)):\n",
    "# #     u, s, vh = svd_drei(A_matrix[i].cpu(), B_matrix[i].cpu(), C_matrix[i].cpu(), U_W4, S_W4, VH_W4.T)\n",
    "#     u, s, vh = svd_drei(A_matrix[i].cpu(), B_matrix[i].cpu(), C_matrix[i].cpu(), model.W1.clone().cpu())\n",
    "#     b = torch.matmul(u[:, :k], torch.matmul(torch.diag_embed(s)[:k, :k], vh[:k, :]))\n",
    "#     B5.append(b)\n",
    "# B5 = torch.stack(B5)\n",
    "# print(np.abs((U_batch2[0].cpu().detach().numpy()-u)).mean())\n",
    "# print(np.abs((B1[0].cpu().detach().numpy()-b)).mean())\n",
    "# print(np.abs((B1.cpu().detach().numpy()-B5_np)).mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(8.3093e-07, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(B_auto-B4).abs().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.4207e-06, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(B_auto - B5).abs().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.4506e-06, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(B4 - B5).abs().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.2689, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(B_auto-B2).abs().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.2647, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(B5_np-B1).abs().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([60])\n",
      "B: tensor(1.3274, grad_fn=<MeanBackward0>)\n",
      "U: tensor(0.0397, grad_fn=<MeanBackward0>)\n",
      "V: tensor(0.0402, grad_fn=<MeanBackward0>)\n",
      "S: tensor(106.7924, grad_fn=<MeanBackward0>)\n",
      "True :S tensor([271.3075, 245.3769, 239.5632, 233.7009, 215.0662, 203.6797, 196.7259,\n",
      "        186.2275, 184.4125, 176.9185, 169.7229, 154.2443, 153.6531, 148.4718,\n",
      "        143.9988, 137.9519, 135.4133, 129.7896, 128.5175, 120.7616, 117.9305,\n",
      "        115.7222, 114.6138, 113.4050, 109.2812, 106.3531, 104.5878, 103.2988,\n",
      "         99.5213,  96.4812], grad_fn=<SliceBackward>)\n",
      "False :S tensor([5.1437e+02, 4.4828e+02, 3.8746e+02, 3.7791e+02, 3.1490e+02, 2.6118e+02,\n",
      "        2.1300e+02, 1.4814e+02, 1.3088e+02, 9.0460e+01, 6.9594e+01, 5.6696e+01,\n",
      "        5.3993e+01, 5.0606e+01, 3.2743e+01, 2.9253e+01, 2.6810e+01, 1.7141e+01,\n",
      "        1.3089e+01, 1.0884e+01, 6.4453e+00, 5.6343e+00, 5.0491e+00, 3.8335e+00,\n",
      "        2.9501e+00, 1.7577e+00, 1.1133e+00, 8.6683e-01, 7.5486e-01, 4.8541e-01],\n",
      "       grad_fn=<SliceBackward>)\n",
      "tensor([  0.5275,   0.5474,   0.6183,   0.6184,   0.6830,   0.7798,   0.9236,\n",
      "          1.2571,   1.4090,   1.9558,   2.4388,   2.7206,   2.8458,   2.9339,\n",
      "          4.3979,   4.7158,   5.0509,   7.5720,   9.8185,  11.0948,  18.2970,\n",
      "         20.5388,  22.7000,  29.5827,  37.0429,  60.5084,  93.9462, 119.1687,\n",
      "        131.8413, 198.7637], grad_fn=<DivBackward0>)\n",
      "torch.Size([60])\n",
      "B: tensor(1.2730, grad_fn=<MeanBackward0>)\n",
      "U: tensor(0.0397, grad_fn=<MeanBackward0>)\n",
      "V: tensor(0.0399, grad_fn=<MeanBackward0>)\n",
      "S: tensor(85.8577, grad_fn=<MeanBackward0>)\n",
      "True :S tensor([276.5725, 246.2196, 243.9737, 235.6584, 214.0355, 207.3168, 201.5729,\n",
      "        192.9784, 169.5884, 166.6889, 158.0877, 154.7402, 148.0128, 144.0289,\n",
      "        140.0008, 139.4454, 133.6513, 128.4162, 125.3756, 119.2741, 116.4571,\n",
      "        114.9335, 110.9875, 108.7153, 107.0327, 103.9133, 102.4091,  99.7203,\n",
      "         96.5328,  95.5052], grad_fn=<SliceBackward>)\n",
      "False :S tensor([538.4501, 359.7264, 326.2608, 286.6251, 276.1275, 241.6319, 186.5708,\n",
      "        149.3285, 132.7502, 128.5612, 105.9281,  89.4653,  75.8326,  68.3111,\n",
      "         46.9310,  46.2651,  39.8190,  32.6078,  21.5453,  17.6504,  15.9464,\n",
      "         10.2812,   9.0059,   7.8988,   7.3389,   4.7922,   3.4876,   2.9501,\n",
      "          2.3950,   1.7192], grad_fn=<SliceBackward>)\n",
      "tensor([ 0.5136,  0.6845,  0.7478,  0.8222,  0.7751,  0.8580,  1.0804,  1.2923,\n",
      "         1.2775,  1.2966,  1.4924,  1.7296,  1.9518,  2.1084,  2.9831,  3.0141,\n",
      "         3.3565,  3.9382,  5.8192,  6.7576,  7.3030, 11.1790, 12.3239, 13.7635,\n",
      "        14.5843, 21.6836, 29.3634, 33.8018, 40.3064, 55.5514],\n",
      "       grad_fn=<DivBackward0>)\n",
      "torch.Size([60])\n",
      "B: tensor(1.5284, grad_fn=<MeanBackward0>)\n",
      "U: tensor(0.0397, grad_fn=<MeanBackward0>)\n",
      "V: tensor(0.0403, grad_fn=<MeanBackward0>)\n",
      "S: tensor(114.0873, grad_fn=<MeanBackward0>)\n",
      "True :S tensor([282.2561, 257.5713, 244.8411, 234.3776, 220.9427, 208.1538, 194.6295,\n",
      "        190.9514, 186.1019, 170.1130, 158.5079, 156.2493, 151.1429, 146.9861,\n",
      "        145.8215, 139.8099, 137.6362, 129.1301, 125.7596, 122.4174, 116.4411,\n",
      "        115.8288, 110.8387, 109.1817, 107.5251, 105.5555, 103.7554, 100.6993,\n",
      "         97.3288,  93.7416], grad_fn=<SliceBackward>)\n",
      "False :S tensor([795.7503, 542.3297, 490.4407, 400.2103, 341.2377, 241.8737, 207.9887,\n",
      "        171.4989, 160.1427,  99.5362,  81.1675,  79.2890,  64.7112,  54.0961,\n",
      "         42.4679,  36.1082,  32.8627,  27.6855,  23.6155,  22.7258,  19.6237,\n",
      "         14.2252,  12.3447,   9.6635,   5.7530,   4.3746,   4.2582,   3.8755,\n",
      "          3.0714,   2.8649], grad_fn=<SliceBackward>)\n",
      "tensor([ 0.3547,  0.4749,  0.4992,  0.5856,  0.6475,  0.8606,  0.9358,  1.1134,\n",
      "         1.1621,  1.7091,  1.9528,  1.9706,  2.3357,  2.7171,  3.4337,  3.8720,\n",
      "         4.1882,  4.6642,  5.3253,  5.3867,  5.9337,  8.1425,  8.9787, 11.2984,\n",
      "        18.6903, 24.1292, 24.3662, 25.9833, 31.6890, 32.7208],\n",
      "       grad_fn=<DivBackward0>)\n",
      "torch.Size([60])\n",
      "B: tensor(1.2547, grad_fn=<MeanBackward0>)\n",
      "U: tensor(0.0398, grad_fn=<MeanBackward0>)\n",
      "V: tensor(0.0384, grad_fn=<MeanBackward0>)\n",
      "S: tensor(88.5191, grad_fn=<MeanBackward0>)\n",
      "True :S tensor([280.6177, 245.3883, 234.2251, 227.2359, 212.1139, 197.7175, 187.8784,\n",
      "        181.1988, 178.9435, 163.5676, 159.2780, 156.4763, 152.2097, 149.5122,\n",
      "        144.9770, 140.1649, 131.8072, 129.8217, 125.2743, 121.9378, 119.7432,\n",
      "        119.6003, 111.9659, 107.5760, 107.4043, 105.6245, 104.0760, 100.2478,\n",
      "         98.8227,  96.1892], grad_fn=<SliceBackward>)\n",
      "False :S tensor([5.6393e+02, 3.4336e+02, 3.1702e+02, 2.9233e+02, 2.2675e+02, 2.0890e+02,\n",
      "        1.8878e+02, 1.6185e+02, 1.1223e+02, 1.0882e+02, 1.0454e+02, 7.2393e+01,\n",
      "        6.5395e+01, 5.0639e+01, 4.6981e+01, 3.3711e+01, 3.2243e+01, 2.7914e+01,\n",
      "        2.2986e+01, 1.7234e+01, 1.2750e+01, 1.1252e+01, 7.2656e+00, 6.1500e+00,\n",
      "        4.7321e+00, 2.6118e+00, 1.9827e+00, 1.4010e+00, 1.1506e+00, 4.9838e-01],\n",
      "       grad_fn=<SliceBackward>)\n",
      "tensor([  0.4976,   0.7147,   0.7388,   0.7773,   0.9355,   0.9465,   0.9952,\n",
      "          1.1195,   1.5944,   1.5031,   1.5236,   2.1615,   2.3276,   2.9525,\n",
      "          3.0859,   4.1579,   4.0879,   4.6507,   5.4501,   7.0752,   9.3919,\n",
      "         10.6296,  15.4104,  17.4921,  22.6971,  40.4408,  52.4920,  71.5567,\n",
      "         85.8903, 193.0025], grad_fn=<DivBackward0>)\n",
      "torch.Size([60])\n",
      "B: tensor(1.2874, grad_fn=<MeanBackward0>)\n",
      "U: tensor(0.0399, grad_fn=<MeanBackward0>)\n",
      "V: tensor(0.0393, grad_fn=<MeanBackward0>)\n",
      "S: tensor(92.7863, grad_fn=<MeanBackward0>)\n",
      "True :S tensor([254.3399, 243.8117, 238.3915, 220.7901, 216.4553, 204.8407, 189.5711,\n",
      "        188.0809, 171.2750, 161.9000, 158.8046, 153.2712, 148.3778, 145.5175,\n",
      "        133.8143, 133.0647, 128.4253, 123.8363, 121.9041, 118.4830, 113.9773,\n",
      "        110.8425, 109.7579, 106.9817, 105.0280, 102.0484, 100.6998,  97.0994,\n",
      "         96.3501,  92.6451], grad_fn=<SliceBackward>)\n",
      "False :S tensor([503.4343, 470.3098, 344.1922, 291.7164, 284.8887, 226.0406, 211.5821,\n",
      "        188.9802, 133.0264, 129.5084,  97.4891,  71.9980,  50.0709,  42.7744,\n",
      "         33.3258,  28.5450,  25.9644,  21.1534,  15.5701,  13.8743,  13.2342,\n",
      "          9.8195,   7.5343,   5.3349,   3.7469,   3.3355,   2.9880,   2.6086,\n",
      "          1.9858,   1.4890], grad_fn=<SliceBackward>)\n",
      "tensor([ 0.5052,  0.5184,  0.6926,  0.7569,  0.7598,  0.9062,  0.8960,  0.9952,\n",
      "         1.2875,  1.2501,  1.6289,  2.1288,  2.9634,  3.4020,  4.0153,  4.6616,\n",
      "         4.9462,  5.8542,  7.8294,  8.5397,  8.6123, 11.2880, 14.5678, 20.0534,\n",
      "        28.0307, 30.5946, 33.7019, 37.2226, 48.5205, 62.2198],\n",
      "       grad_fn=<DivBackward0>)\n",
      "torch.Size([60])\n",
      "B: tensor(1.1933, grad_fn=<MeanBackward0>)\n",
      "U: tensor(0.0398, grad_fn=<MeanBackward0>)\n",
      "V: tensor(0.0412, grad_fn=<MeanBackward0>)\n",
      "S: tensor(89.0274, grad_fn=<MeanBackward0>)\n",
      "True :S tensor([279.7749, 259.8912, 242.2190, 240.1649, 217.8789, 214.0564, 193.8200,\n",
      "        186.0091, 174.7869, 172.0061, 165.6434, 161.4179, 153.8666, 148.4578,\n",
      "        143.6769, 141.5433, 136.1020, 131.9903, 130.2322, 123.3485, 121.4637,\n",
      "        118.0839, 113.0005, 111.5757, 107.1170, 103.7177, 103.0012, 101.5763,\n",
      "         97.1788,  96.9330], grad_fn=<SliceBackward>)\n",
      "False :S tensor([415.1609, 369.9289, 348.8972, 254.3749, 224.9806, 149.0804, 135.2407,\n",
      "        122.8793, 108.9795,  88.2334,  75.9148,  71.5103,  66.2445,  56.9347,\n",
      "         51.9338,  40.4434,  32.2282,  29.7990,  28.1914,  22.0546,  18.3368,\n",
      "         13.5300,  12.1241,   8.3903,   7.5491,   5.0526,   3.3350,   2.5814,\n",
      "          1.4058,   1.2229], grad_fn=<SliceBackward>)\n",
      "tensor([ 0.6739,  0.7025,  0.6942,  0.9441,  0.9684,  1.4358,  1.4331,  1.5138,\n",
      "         1.6039,  1.9494,  2.1820,  2.2573,  2.3227,  2.6075,  2.7665,  3.4998,\n",
      "         4.2231,  4.4294,  4.6196,  5.5929,  6.6241,  8.7275,  9.3203, 13.2981,\n",
      "        14.1894, 20.5277, 30.8853, 39.3492, 69.1248, 79.2650],\n",
      "       grad_fn=<DivBackward0>)\n",
      "torch.Size([60])\n",
      "B: tensor(1.3529, grad_fn=<MeanBackward0>)\n",
      "U: tensor(0.0398, grad_fn=<MeanBackward0>)\n",
      "V: tensor(0.0403, grad_fn=<MeanBackward0>)\n",
      "S: tensor(94.5376, grad_fn=<MeanBackward0>)\n",
      "True :S tensor([295.2014, 259.6170, 242.4574, 234.2837, 217.2261, 205.2446, 193.7404,\n",
      "        184.9751, 180.0477, 170.1574, 167.1729, 157.3521, 149.2135, 147.7722,\n",
      "        143.3954, 140.9859, 132.0856, 130.8099, 125.6446, 120.4283, 117.1045,\n",
      "        112.6266, 109.5829, 108.9574, 106.6327, 102.9443, 101.3252,  99.4833,\n",
      "         98.1196,  97.3220], grad_fn=<SliceBackward>)\n",
      "False :S tensor([4.8957e+02, 4.4734e+02, 4.3166e+02, 3.1718e+02, 3.0554e+02, 2.7641e+02,\n",
      "        2.4255e+02, 1.9331e+02, 1.7744e+02, 1.4161e+02, 1.3204e+02, 8.4169e+01,\n",
      "        7.2172e+01, 6.5756e+01, 4.5323e+01, 3.2012e+01, 2.5901e+01, 2.1046e+01,\n",
      "        1.5449e+01, 1.3923e+01, 8.3356e+00, 5.9568e+00, 4.2903e+00, 3.0423e+00,\n",
      "        2.7624e+00, 9.1358e-01, 7.2750e-01, 4.9827e-01, 2.5646e-01, 2.2290e-01],\n",
      "       grad_fn=<SliceBackward>)\n",
      "tensor([  0.6030,   0.5804,   0.5617,   0.7386,   0.7110,   0.7425,   0.7988,\n",
      "          0.9569,   1.0147,   1.2016,   1.2661,   1.8695,   2.0675,   2.2473,\n",
      "          3.1639,   4.4041,   5.0995,   6.2154,   8.1327,   8.6495,  14.0487,\n",
      "         18.9072,  25.5422,  35.8141,  38.6020, 112.6827, 139.2790, 199.6563,\n",
      "        382.5931, 436.6100], grad_fn=<DivBackward0>)\n",
      "torch.Size([60])\n",
      "B: tensor(1.5887, grad_fn=<MeanBackward0>)\n",
      "U: tensor(0.0397, grad_fn=<MeanBackward0>)\n",
      "V: tensor(0.0398, grad_fn=<MeanBackward0>)\n",
      "S: tensor(112.2843, grad_fn=<MeanBackward0>)\n",
      "True :S tensor([286.0740, 257.5467, 244.7480, 243.9073, 228.4018, 203.1112, 199.1245,\n",
      "        198.0248, 186.8788, 178.1988, 163.1165, 158.8513, 149.3488, 144.8109,\n",
      "        141.8701, 137.3275, 134.4009, 131.3932, 127.6983, 123.5994, 120.2552,\n",
      "        118.5892, 115.7733, 115.0184, 109.4794, 106.4810, 105.2357, 102.4743,\n",
      "         99.9019,  96.4869], grad_fn=<SliceBackward>)\n",
      "False :S tensor([612.6960, 529.7797, 483.2144, 462.0959, 365.7444, 336.1663, 324.6273,\n",
      "        279.3960, 260.1912, 210.3223, 171.6424, 139.5072, 122.7969, 106.0751,\n",
      "         59.3019,  47.7600,  30.8167,  22.8952,  19.7549,  14.4461,  10.7517,\n",
      "          9.4675,   8.6496,   7.1720,   4.8705,   3.6154,   3.0394,   2.2993,\n",
      "          2.1796,   1.8103], grad_fn=<SliceBackward>)\n",
      "tensor([ 0.4669,  0.4861,  0.5065,  0.5278,  0.6245,  0.6042,  0.6134,  0.7088,\n",
      "         0.7182,  0.8473,  0.9503,  1.1387,  1.2162,  1.3652,  2.3923,  2.8754,\n",
      "         4.3613,  5.7389,  6.4641,  8.5559, 11.1848, 12.5260, 13.3848, 16.0371,\n",
      "        22.4779, 29.4524, 34.6242, 44.5674, 45.8359, 53.2983],\n",
      "       grad_fn=<DivBackward0>)\n",
      "torch.Size([60])\n",
      "B: tensor(1.2874, grad_fn=<MeanBackward0>)\n",
      "U: tensor(0.0399, grad_fn=<MeanBackward0>)\n",
      "V: tensor(0.0413, grad_fn=<MeanBackward0>)\n",
      "S: tensor(101.0419, grad_fn=<MeanBackward0>)\n",
      "True :S tensor([259.9533, 254.6122, 242.7186, 231.5828, 220.7866, 204.8799, 193.5988,\n",
      "        183.9877, 172.1984, 169.8558, 160.7328, 156.6560, 150.6846, 149.8383,\n",
      "        145.9070, 136.4587, 132.0543, 131.5911, 125.5907, 120.1720, 115.8107,\n",
      "        114.4676, 111.1909, 107.8097, 107.4285, 105.2641, 101.9344,  99.8836,\n",
      "         97.2537,  95.9998], grad_fn=<SliceBackward>)\n",
      "False :S tensor([492.2703, 412.7724, 383.4416, 314.8265, 301.5059, 268.4541, 214.6006,\n",
      "        173.7262, 130.6296,  96.9948,  58.5833,  51.4581,  47.5177,  38.3690,\n",
      "         30.1616,  25.0640,  17.8362,  14.4900,  12.3098,   9.5536,   6.8096,\n",
      "          5.7291,   5.1458,   4.6209,   3.1152,   2.7081,   2.5080,   1.7954,\n",
      "          1.0695,   1.0565], grad_fn=<SliceBackward>)\n",
      "tensor([ 0.5281,  0.6168,  0.6330,  0.7356,  0.7323,  0.7632,  0.9021,  1.0591,\n",
      "         1.3182,  1.7512,  2.7437,  3.0443,  3.1711,  3.9052,  4.8375,  5.4444,\n",
      "         7.4037,  9.0815, 10.2025, 12.5787, 17.0069, 19.9799, 21.6079, 23.3309,\n",
      "        34.4858, 38.8698, 40.6439, 55.6338, 90.9356, 90.8671],\n",
      "       grad_fn=<DivBackward0>)\n",
      "torch.Size([60])\n",
      "B: tensor(1.1487, grad_fn=<MeanBackward0>)\n",
      "U: tensor(0.0399, grad_fn=<MeanBackward0>)\n",
      "V: tensor(0.0407, grad_fn=<MeanBackward0>)\n",
      "S: tensor(95.2223, grad_fn=<MeanBackward0>)\n",
      "True :S tensor([264.0835, 250.0458, 241.6072, 234.0419, 220.2778, 202.5058, 198.0203,\n",
      "        183.6677, 179.8958, 169.4959, 163.4814, 160.0550, 149.3304, 146.2055,\n",
      "        142.4486, 138.3870, 137.7239, 129.0513, 125.6873, 122.0433, 119.4141,\n",
      "        116.4992, 114.8041, 111.4815, 110.0986, 105.9796, 104.7342, 101.2015,\n",
      "         98.5236,  97.2628], grad_fn=<SliceBackward>)\n",
      "False :S tensor([5.1076e+02, 3.1460e+02, 2.5830e+02, 2.0571e+02, 2.0353e+02, 1.7527e+02,\n",
      "        1.1953e+02, 1.1140e+02, 9.7375e+01, 7.4777e+01, 7.0882e+01, 5.5724e+01,\n",
      "        4.3673e+01, 3.9984e+01, 3.4280e+01, 3.1437e+01, 2.1521e+01, 1.8086e+01,\n",
      "        1.2965e+01, 1.0721e+01, 8.2900e+00, 6.8375e+00, 3.9087e+00, 2.2298e+00,\n",
      "        2.0791e+00, 1.5397e+00, 8.6225e-01, 4.0847e-01, 3.7994e-01, 1.6191e-01],\n",
      "       grad_fn=<SliceBackward>)\n",
      "tensor([5.1704e-01, 7.9480e-01, 9.3539e-01, 1.1377e+00, 1.0823e+00, 1.1554e+00,\n",
      "        1.6567e+00, 1.6487e+00, 1.8474e+00, 2.2667e+00, 2.3064e+00, 2.8723e+00,\n",
      "        3.4193e+00, 3.6566e+00, 4.1554e+00, 4.4020e+00, 6.3996e+00, 7.1356e+00,\n",
      "        9.6940e+00, 1.1383e+01, 1.4405e+01, 1.7038e+01, 2.9372e+01, 4.9995e+01,\n",
      "        5.2955e+01, 6.8830e+01, 1.2147e+02, 2.4776e+02, 2.5931e+02, 6.0071e+02],\n",
      "       grad_fn=<DivBackward0>)\n",
      "torch.Size([60])\n",
      "B: tensor(1.3096, grad_fn=<MeanBackward0>)\n",
      "U: tensor(0.0396, grad_fn=<MeanBackward0>)\n",
      "V: tensor(0.0406, grad_fn=<MeanBackward0>)\n",
      "S: tensor(96.7171, grad_fn=<MeanBackward0>)\n",
      "True :S tensor([258.9375, 243.5279, 236.1239, 227.2123, 216.6259, 207.1143, 203.3778,\n",
      "        184.6367, 175.9730, 162.1253, 156.9079, 152.0063, 149.7934, 142.1556,\n",
      "        138.2998, 134.4211, 131.6814, 124.5266, 122.4896, 118.6427, 115.6664,\n",
      "        111.7740, 107.9214, 105.8044, 103.3343, 101.5659,  99.4354,  96.9303,\n",
      "         95.3040,  93.0068], grad_fn=<SliceBackward>)\n",
      "False :S tensor([640.3083, 387.1628, 342.1541, 326.1027, 264.8519, 221.6002, 191.2399,\n",
      "        157.4648, 141.9592, 122.9611,  79.0209,  63.7463,  51.5986,  47.3228,\n",
      "         32.3191,  23.5011,  19.3287,  16.4790,  14.0510,  12.4274,  10.4129,\n",
      "          9.0566,   6.2700,   5.5233,   3.7011,   2.8859,   2.7614,   2.1658,\n",
      "          1.4564,   1.2517], grad_fn=<SliceBackward>)\n",
      "tensor([ 0.4044,  0.6290,  0.6901,  0.6968,  0.8179,  0.9346,  1.0635,  1.1726,\n",
      "         1.2396,  1.3185,  1.9857,  2.3846,  2.9030,  3.0040,  4.2792,  5.7198,\n",
      "         6.8127,  7.5567,  8.7175,  9.5469, 11.1080, 12.3417, 17.2125, 19.1559,\n",
      "        27.9201, 35.1942, 36.0097, 44.7551, 65.4369, 74.3067],\n",
      "       grad_fn=<DivBackward0>)\n",
      "torch.Size([60])\n",
      "B: tensor(1.1253, grad_fn=<MeanBackward0>)\n",
      "U: tensor(0.0399, grad_fn=<MeanBackward0>)\n",
      "V: tensor(0.0395, grad_fn=<MeanBackward0>)\n",
      "S: tensor(80.9898, grad_fn=<MeanBackward0>)\n",
      "True :S tensor([259.2193, 250.2723, 246.2592, 221.6466, 213.7034, 192.2734, 189.7793,\n",
      "        183.7405, 174.5600, 162.2086, 153.7222, 151.3500, 146.3732, 144.5856,\n",
      "        141.3494, 133.5166, 130.1741, 127.2509, 125.1490, 119.1100, 118.1042,\n",
      "        113.5685, 112.6098, 109.6812, 106.2837, 103.9738, 101.9625, 101.3637,\n",
      "         99.0108,  98.7173], grad_fn=<SliceBackward>)\n",
      "False :S tensor([455.4557, 282.9311, 247.9142, 212.3944, 193.1322, 178.2088, 163.2106,\n",
      "        135.2624, 118.8675, 103.4514,  81.3545,  75.8310,  69.4559,  53.4959,\n",
      "         37.9346,  35.2024,  24.4844,  23.3737,  21.9231,  14.1470,  13.3463,\n",
      "          6.4194,   4.4097,   3.5816,   2.9179,   1.2541,   1.1640,   0.7595,\n",
      "          0.5523,   0.4888], grad_fn=<SliceBackward>)\n",
      "tensor([  0.5691,   0.8846,   0.9933,   1.0436,   1.1065,   1.0789,   1.1628,\n",
      "          1.3584,   1.4685,   1.5680,   1.8895,   1.9959,   2.1074,   2.7027,\n",
      "          3.7261,   3.7928,   5.3166,   5.4442,   5.7085,   8.4194,   8.8492,\n",
      "         17.6915,  25.5370,  30.6232,  36.4253,  82.9079,  87.5957, 133.4660,\n",
      "        179.2543, 201.9730], grad_fn=<DivBackward0>)\n",
      "torch.Size([60])\n",
      "B: tensor(1.0367, grad_fn=<MeanBackward0>)\n",
      "U: tensor(0.0398, grad_fn=<MeanBackward0>)\n",
      "V: tensor(0.0409, grad_fn=<MeanBackward0>)\n",
      "S: tensor(105.3347, grad_fn=<MeanBackward0>)\n",
      "True :S tensor([265.6217, 252.6029, 243.8192, 236.2047, 208.0412, 203.8103, 194.0915,\n",
      "        183.1648, 175.4030, 169.6517, 166.8530, 159.3740, 151.1123, 146.7485,\n",
      "        140.6725, 135.3204, 132.6389, 130.8733, 127.0510, 120.6633, 114.8588,\n",
      "        113.3557, 110.9015, 108.0959, 107.0945, 104.7755,  99.0398,  98.3185,\n",
      "         96.3159,  93.1224], grad_fn=<SliceBackward>)\n",
      "False :S tensor([356.8898, 324.2605, 190.2931, 158.9556, 120.6765,  96.3808,  79.5829,\n",
      "         61.2057,  54.9706,  46.0754,  41.4409,  33.5360,  28.3272,  25.0840,\n",
      "         20.8286,  18.9588,  17.1530,  13.6017,  12.4373,  11.4432,  10.6210,\n",
      "          7.7485,   6.0897,   4.9241,   3.2282,   2.9417,   2.4677,   2.0247,\n",
      "          1.7250,   1.5336], grad_fn=<SliceBackward>)\n",
      "tensor([ 0.7443,  0.7790,  1.2813,  1.4860,  1.7240,  2.1146,  2.4389,  2.9926,\n",
      "         3.1908,  3.6820,  4.0263,  4.7523,  5.3345,  5.8503,  6.7538,  7.1376,\n",
      "         7.7327,  9.6218, 10.2154, 10.5445, 10.8143, 14.6293, 18.2115, 21.9522,\n",
      "        33.1746, 35.6170, 40.1343, 48.5595, 55.8362, 60.7222],\n",
      "       grad_fn=<DivBackward0>)\n",
      "torch.Size([60])\n",
      "B: tensor(1.3133, grad_fn=<MeanBackward0>)\n",
      "U: tensor(0.0399, grad_fn=<MeanBackward0>)\n",
      "V: tensor(0.0400, grad_fn=<MeanBackward0>)\n",
      "S: tensor(85.5838, grad_fn=<MeanBackward0>)\n",
      "True :S tensor([276.4006, 243.7852, 239.6095, 235.1505, 221.0791, 208.6870, 192.7902,\n",
      "        183.7821, 173.9586, 168.1475, 165.4167, 154.1874, 150.7836, 148.2626,\n",
      "        144.5936, 137.0196, 133.9826, 129.2013, 125.3008, 122.4991, 116.9069,\n",
      "        116.1869, 110.7568, 109.2950, 104.7256, 101.8128, 101.1888,  99.5777,\n",
      "         97.3389,  95.9240], grad_fn=<SliceBackward>)\n",
      "False :S tensor([4.7895e+02, 3.9843e+02, 3.8488e+02, 3.3115e+02, 2.9013e+02, 2.2595e+02,\n",
      "        2.1352e+02, 1.9860e+02, 1.8382e+02, 1.4858e+02, 1.1621e+02, 1.1040e+02,\n",
      "        1.0384e+02, 8.2454e+01, 6.9783e+01, 5.5073e+01, 2.7321e+01, 2.2381e+01,\n",
      "        1.7047e+01, 1.2599e+01, 9.7611e+00, 6.8564e+00, 5.6637e+00, 2.1494e+00,\n",
      "        1.9029e+00, 1.5731e+00, 1.0505e+00, 5.3316e-01, 3.2018e-01, 2.7958e-01],\n",
      "       grad_fn=<SliceBackward>)\n",
      "tensor([  0.5771,   0.6119,   0.6226,   0.7101,   0.7620,   0.9236,   0.9029,\n",
      "          0.9254,   0.9464,   1.1317,   1.4234,   1.3966,   1.4520,   1.7981,\n",
      "          2.0721,   2.4880,   4.9039,   5.7728,   7.3504,   9.7230,  11.9769,\n",
      "         16.9459,  19.5557,  50.8485,  55.0346,  64.7199,  96.3246, 186.7682,\n",
      "        304.0106, 343.1003], grad_fn=<DivBackward0>)\n",
      "torch.Size([60])\n",
      "B: tensor(1.5744, grad_fn=<MeanBackward0>)\n",
      "U: tensor(0.0398, grad_fn=<MeanBackward0>)\n",
      "V: tensor(0.0406, grad_fn=<MeanBackward0>)\n",
      "S: tensor(104.8026, grad_fn=<MeanBackward0>)\n",
      "True :S tensor([283.0291, 269.5540, 241.2215, 239.4334, 217.9893, 209.5388, 198.7279,\n",
      "        189.5442, 173.0776, 170.0389, 164.3535, 155.7665, 151.9879, 147.9899,\n",
      "        142.4543, 139.1551, 137.3822, 135.6447, 127.0185, 123.2547, 119.7541,\n",
      "        116.1954, 113.2885, 110.8144, 107.3457, 105.5736, 103.6922, 101.6301,\n",
      "         98.5646,  98.0664], grad_fn=<SliceBackward>)\n",
      "False :S tensor([672.7035, 545.3016, 481.6284, 416.1224, 402.3590, 310.3447, 257.6825,\n",
      "        245.4136, 206.1273, 184.5198, 170.8004, 157.8768, 127.7290,  90.1556,\n",
      "         66.6463,  61.4423,  51.1467,  45.0204,  39.2231,  22.6684,  19.3034,\n",
      "         10.8829,   8.9537,   7.9496,   6.5619,   4.8573,   3.4671,   3.0725,\n",
      "          2.8979,   2.3599], grad_fn=<SliceBackward>)\n",
      "tensor([ 0.4207,  0.4943,  0.5008,  0.5754,  0.5418,  0.6752,  0.7712,  0.7723,\n",
      "         0.8397,  0.9215,  0.9623,  0.9866,  1.1899,  1.6415,  2.1375,  2.2648,\n",
      "         2.6860,  3.0130,  3.2384,  5.4373,  6.2038, 10.6769, 12.6527, 13.9397,\n",
      "        16.3589, 21.7350, 29.9078, 33.0770, 34.0120, 41.5549],\n",
      "       grad_fn=<DivBackward0>)\n",
      "torch.Size([60])\n",
      "B: tensor(1.3369, grad_fn=<MeanBackward0>)\n",
      "U: tensor(0.0402, grad_fn=<MeanBackward0>)\n",
      "V: tensor(0.0400, grad_fn=<MeanBackward0>)\n",
      "S: tensor(87.7747, grad_fn=<MeanBackward0>)\n",
      "True :S tensor([258.5302, 257.0595, 249.4074, 233.1922, 213.8727, 205.2804, 197.2110,\n",
      "        186.4337, 177.5626, 168.8050, 160.5927, 160.5010, 145.4731, 142.9742,\n",
      "        141.3510, 137.7860, 133.7900, 127.2596, 125.7878, 120.1979, 115.8754,\n",
      "        114.8351, 111.1468, 109.5921, 106.9470, 104.9845, 102.4889, 101.8997,\n",
      "         98.3437,  96.4878], grad_fn=<SliceBackward>)\n",
      "False :S tensor([545.0363, 420.3794, 369.2701, 358.0630, 296.8343, 229.9213, 209.1021,\n",
      "        185.2994, 182.4742, 144.8464, 128.2102,  95.8350,  89.1946,  61.3697,\n",
      "         45.6984,  40.3515,  29.0183,  28.4717,  26.0467,  23.7288,  19.8968,\n",
      "         17.6931,  13.9835,  12.4588,  10.8552,   8.7160,   5.8575,   4.8987,\n",
      "          3.7168,   3.1291], grad_fn=<SliceBackward>)\n",
      "tensor([ 0.4743,  0.6115,  0.6754,  0.6513,  0.7205,  0.8928,  0.9431,  1.0061,\n",
      "         0.9731,  1.1654,  1.2526,  1.6748,  1.6310,  2.3297,  3.0931,  3.4146,\n",
      "         4.6105,  4.4697,  4.8293,  5.0655,  5.8238,  6.4904,  7.9484,  8.7964,\n",
      "         9.8522, 12.0450, 17.4969, 20.8015, 26.4595, 30.8353],\n",
      "       grad_fn=<DivBackward0>)\n",
      "torch.Size([60])\n",
      "B: tensor(1.4077, grad_fn=<MeanBackward0>)\n",
      "U: tensor(0.0397, grad_fn=<MeanBackward0>)\n",
      "V: tensor(0.0404, grad_fn=<MeanBackward0>)\n",
      "S: tensor(89.6913, grad_fn=<MeanBackward0>)\n",
      "True :S tensor([266.8969, 259.3027, 240.5467, 230.6809, 215.0388, 207.9566, 189.7099,\n",
      "        183.1751, 180.2454, 165.3448, 161.6488, 156.0481, 154.5014, 143.2629,\n",
      "        141.5928, 133.9773, 130.3398, 128.2043, 123.7120, 121.7019, 120.5162,\n",
      "        116.9328, 114.0589, 110.7620, 109.5468, 104.7499, 103.2147, 100.5428,\n",
      "         99.6419,  95.7153], grad_fn=<SliceBackward>)\n",
      "False :S tensor([521.1397, 464.8590, 431.1488, 359.1890, 315.4692, 266.5897, 258.8186,\n",
      "        221.1382, 190.1615, 167.6608, 148.6423, 130.0919, 106.0141,  93.1198,\n",
      "         75.4989,  64.4982,  52.7771,  41.6377,  31.8712,  25.4410,  21.8755,\n",
      "         14.0955,   8.5023,   7.6496,   4.5010,   4.1038,   2.8022,   2.1018,\n",
      "          1.0483,   0.9363], grad_fn=<SliceBackward>)\n",
      "tensor([  0.5121,   0.5578,   0.5579,   0.6422,   0.6816,   0.7801,   0.7330,\n",
      "          0.8283,   0.9479,   0.9862,   1.0875,   1.1995,   1.4574,   1.5385,\n",
      "          1.8754,   2.0772,   2.4696,   3.0790,   3.8816,   4.7837,   5.5092,\n",
      "          8.2958,  13.4151,  14.4795,  24.3385,  25.5251,  36.8330,  47.8364,\n",
      "         95.0481, 102.2229], grad_fn=<DivBackward0>)\n",
      "torch.Size([60])\n",
      "B: tensor(1.3861, grad_fn=<MeanBackward0>)\n",
      "U: tensor(0.0397, grad_fn=<MeanBackward0>)\n",
      "V: tensor(0.0405, grad_fn=<MeanBackward0>)\n",
      "S: tensor(91.0476, grad_fn=<MeanBackward0>)\n",
      "True :S tensor([275.3020, 254.3944, 244.0359, 232.5647, 209.6642, 201.5696, 195.1688,\n",
      "        187.8603, 182.0405, 169.5521, 158.0918, 151.3543, 149.1091, 145.3560,\n",
      "        142.8740, 139.0749, 136.0980, 127.4391, 127.3519, 121.4191, 119.9236,\n",
      "        114.9647, 111.7583, 108.6698, 107.8925, 105.2626, 101.8335,  99.8331,\n",
      "         95.0196,  93.4539], grad_fn=<SliceBackward>)\n",
      "False :S tensor([572.1425, 455.7034, 433.0751, 325.9075, 298.4343, 264.9532, 237.2299,\n",
      "        195.4716, 174.0778, 146.9798, 135.1219, 113.4783, 100.0814,  81.5911,\n",
      "         70.5944,  49.3668,  43.9762,  31.7261,  28.1004,  24.8709,  17.1825,\n",
      "         11.1623,   8.6614,   8.3121,   4.0693,   2.9603,   2.6517,   1.6422,\n",
      "          1.4196,   1.2750], grad_fn=<SliceBackward>)\n",
      "tensor([ 0.4812,  0.5582,  0.5635,  0.7136,  0.7025,  0.7608,  0.8227,  0.9611,\n",
      "         1.0457,  1.1536,  1.1700,  1.3338,  1.4899,  1.7815,  2.0239,  2.8172,\n",
      "         3.0948,  4.0168,  4.5320,  4.8820,  6.9794, 10.2994, 12.9030, 13.0737,\n",
      "        26.5138, 35.5582, 38.4034, 60.7916, 66.9345, 73.2971],\n",
      "       grad_fn=<DivBackward0>)\n",
      "torch.Size([60])\n",
      "B: tensor(1.1189, grad_fn=<MeanBackward0>)\n",
      "U: tensor(0.0402, grad_fn=<MeanBackward0>)\n",
      "V: tensor(0.0403, grad_fn=<MeanBackward0>)\n",
      "S: tensor(107.1152, grad_fn=<MeanBackward0>)\n",
      "True :S tensor([285.1833, 260.4564, 243.3867, 231.3712, 219.5697, 206.6454, 198.2243,\n",
      "        186.3361, 183.4990, 169.9482, 167.1744, 161.0813, 154.7429, 150.7719,\n",
      "        137.8299, 136.9512, 133.7049, 132.2276, 125.4936, 124.0895, 118.0670,\n",
      "        114.9631, 113.7226, 112.5609, 107.8575, 107.6220, 104.2803, 101.2738,\n",
      "         99.5311,  96.9664], grad_fn=<SliceBackward>)\n",
      "False :S tensor([4.4144e+02, 3.3959e+02, 2.9837e+02, 1.7051e+02, 1.5364e+02, 1.1935e+02,\n",
      "        9.4060e+01, 8.0723e+01, 6.1001e+01, 6.0224e+01, 5.2634e+01, 3.4899e+01,\n",
      "        2.7952e+01, 2.3507e+01, 1.8746e+01, 1.7696e+01, 1.2274e+01, 9.9707e+00,\n",
      "        9.0905e+00, 7.6770e+00, 5.2043e+00, 3.8448e+00, 3.2374e+00, 2.0556e+00,\n",
      "        1.6147e+00, 1.3398e+00, 8.6714e-01, 5.3051e-01, 4.1931e-01, 3.5217e-01],\n",
      "       grad_fn=<SliceBackward>)\n",
      "tensor([  0.6460,   0.7670,   0.8157,   1.3569,   1.4291,   1.7315,   2.1074,\n",
      "          2.3083,   3.0081,   2.8219,   3.1762,   4.6156,   5.5359,   6.4138,\n",
      "          7.3524,   7.7389,  10.8933,  13.2617,  13.8049,  16.1638,  22.6864,\n",
      "         29.9007,  35.1283,  54.7589,  66.7988,  80.3246, 120.2579, 190.9000,\n",
      "        237.3681, 275.3360], grad_fn=<DivBackward0>)\n",
      "torch.Size([60])\n",
      "B: tensor(1.3475, grad_fn=<MeanBackward0>)\n",
      "U: tensor(0.0398, grad_fn=<MeanBackward0>)\n",
      "V: tensor(0.0402, grad_fn=<MeanBackward0>)\n",
      "S: tensor(93.5576, grad_fn=<MeanBackward0>)\n",
      "True :S tensor([277.9687, 261.1838, 236.4302, 232.9856, 207.4969, 202.6379, 197.8760,\n",
      "        180.0887, 167.2547, 164.7366, 161.4644, 158.1897, 153.7001, 143.5827,\n",
      "        140.0950, 136.4914, 130.7281, 130.1639, 126.6014, 125.7855, 119.5730,\n",
      "        116.6424, 115.9261, 110.1427, 109.4248, 107.9919, 103.3204, 102.7909,\n",
      "         99.5957,  99.0489], grad_fn=<SliceBackward>)\n",
      "False :S tensor([576.8303, 402.6889, 382.8584, 320.1360, 268.7716, 256.0560, 235.8411,\n",
      "        194.7016, 159.1073, 145.3860,  99.6255,  92.3849,  64.9874,  59.0204,\n",
      "         48.8484,  40.9261,  27.2723,  22.8244,  18.9225,  15.6598,  13.0056,\n",
      "         12.2751,   9.8523,   9.1520,   5.5458,   4.3715,   2.9971,   2.3091,\n",
      "          2.0423,   1.2237], grad_fn=<SliceBackward>)\n",
      "tensor([ 0.4819,  0.6486,  0.6175,  0.7278,  0.7720,  0.7914,  0.8390,  0.9249,\n",
      "         1.0512,  1.1331,  1.6207,  1.7123,  2.3651,  2.4328,  2.8680,  3.3351,\n",
      "         4.7934,  5.7029,  6.6905,  8.0324,  9.1940,  9.5024, 11.7664, 12.0348,\n",
      "        19.7313, 24.7038, 34.4738, 44.5156, 48.7668, 80.9439],\n",
      "       grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "A_matrix = []\n",
    "B_matrix = []\n",
    "C_matrix = []\n",
    "for i in range(imgs.shape[0]): \n",
    "    diag_sigma_prime1 = torch.diag( torch.mul(1.0 - code_data1[i], code_data1[i]))\n",
    "    grad_1 = torch.matmul(model.W1.t(), diag_sigma_prime1)\n",
    "\n",
    "    diag_sigma_prime2 = torch.diag( torch.mul(1.0 - code_data2[i], code_data2[i]))\n",
    "    grad_2 = torch.matmul(model.W2.t(), diag_sigma_prime2)\n",
    "\n",
    "    diag_sigma_prime3  = torch.diag( torch.mul(1.0 - code_data3[i], code_data3[i]))\n",
    "    grad_3 = torch.matmul(model.W2, diag_sigma_prime3)\n",
    "\n",
    "    A_matrix.append(grad_1)\n",
    "    B_matrix.append(grad_2)\n",
    "    C_matrix.append(grad_3)\n",
    "    \n",
    "A_matrix = torch.stack(A_matrix)\n",
    "B_matrix = torch.stack(B_matrix)\n",
    "C_matrix = torch.stack(C_matrix)\n",
    "            \n",
    "def svd_product(A, U, S, VH): # A*U*S*VH\n",
    "    Q, R = torch.qr(torch.matmul(A, U))\n",
    "    u_temp, s_temp, vh_temp = torch.svd(torch.matmul(R, torch.diag(S)))\n",
    "    return [torch.matmul(Q, u_temp), s_temp, torch.matmul(vh_temp.T,VH)]\n",
    "\n",
    "def svd_drei(A, B, C, D): # A*B*C*D\n",
    "    U_temp, S_temp, VH_temp = torch.svd(torch.matmul(C, D))\n",
    "    print(S_temp.shape)\n",
    "    return svd_product(torch.matmul(A, B), U_temp, S_temp, VH_temp.T)\n",
    "\n",
    "# def svd_drei(A, B, C, U, S, VH): # A*B*C*U*S*VH\n",
    "#     U1, S1, VH1 = svd_product(C, U, S, VH)\n",
    "#     U2, S2, VH2 = svd_product(B, U1, S1, VH1)\n",
    "#     return svd_product(A, U2, S2, VH2)\n",
    "# def svd_product_np(A, U, S, VH): # A*U*S*VH\n",
    "    \n",
    "\n",
    "# def svd_drei_np(A, B, C, D): # A*B*C*U*S*VH\n",
    "#     U, S, VH = np.linalg.svd(np.matmul(C, D), full_matrices=False)\n",
    "#     Q, R = np.linalg.qr(np.matmul(np.matmul(A, B), U))\n",
    "#     u_temp, s_temp, vh_temp = np.linalg.svd(np.matmul(R, np.diag(S)), full_matrices=False)\n",
    "#     return [np.matmul(Q,u_temp), s_temp, np.matmul(vh_temp, VH)]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "B5_np=[]\n",
    "# U_W4, S_W4, VH_W4 = torch.svd(model.W1.clone().cpu())\n",
    "for i in range(len(A_matrix)):\n",
    "#     u, s, vh = svd_drei(A_matrix[i].cpu(), B_matrix[i].cpu(), C_matrix[i].cpu(), U_W4, S_W4, VH_W4.T)\n",
    "    u, s, vh = svd_drei(A_matrix[i].cpu(), B_matrix[i].cpu(), C_matrix[i].cpu(), model.W1.clone().cpu())\n",
    "    b = torch.matmul(u[:, :k], torch.matmul(torch.diag(s)[:k, :k], vh[:k, :]))\n",
    "    print(\"B:\", (B4[i].cpu()-b).abs().mean())\n",
    "    print(\"U:\", (U_batch4[i,:, :k].cpu()-u[:, :k]).abs().mean())\n",
    "    print(\"V:\", (torch.transpose(V_batch4[i,:, :k],0,1).cpu()-vh[:k, :]).abs().mean())\n",
    "    print(\"S:\", (S_batch4[i,:k].cpu()-s[:k]).abs().mean())\n",
    "    print(\"True :S\", S_batch4[i,:k].cpu())\n",
    "    print(\"False :S\", s[:k])\n",
    "    print(S_batch4[i,:k].cpu()/s[:k])\n",
    "#     break\n",
    "\n",
    "#     B5_np.append(b)\n",
    "# B5_np = np.stack(B5_np)\n",
    "\n",
    "# B5=[]\n",
    "# U_W4, S_W4, VH_W4 = torch.svd(model.W1.clone().cpu())\n",
    "\n",
    "# U_ = []\n",
    "# S_ = []\n",
    "# V_ = []\n",
    "# for i in range(len(A_matrix)):\n",
    "# #     u, s, vh = svd_drei(A_matrix[i].cpu(), B_matrix[i].cpu(), C_matrix[i].cpu(), U_W4, S_W4, VH_W4.T)\n",
    "#     u, s, vh = svd_drei(A_matrix[i].cpu(), B_matrix[i].cpu(), C_matrix[i].cpu(), model.W1.clone().cpu())\n",
    "#     b = torch.matmul(u[:, :k], torch.matmul(torch.diag_embed(s)[:k, :k], vh[:k, :]))\n",
    "#     B5.append(b)\n",
    "# B5 = torch.stack(B5)\n",
    "# print(np.abs((U_batch2[0].cpu().detach().numpy()-u)).mean())\n",
    "# print(np.abs((B1[0].cpu().detach().numpy()-b)).mean())\n",
    "# print(np.abs((B1.cpu().detach().numpy()-B5_np)).mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Q' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-98-47e0fe0419cf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mQ\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'Q' is not defined"
     ]
    }
   ],
   "source": [
    "np.matmul(np.matmul(Q,u), np.matmul(np.diag(s), np.matmul(vh, VH)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([60])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-18.7188,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "           0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "           0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "           0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "           0.0000,   0.0000],\n",
       "        [  0.0000,   5.7563,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "           0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "           0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "           0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "           0.0000,   0.0000],\n",
       "        [  0.0000,   0.0000, -15.6516,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "           0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "           0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "           0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "           0.0000,   0.0000],\n",
       "        [  0.0000,   0.0000,   0.0000,   6.6253,   0.0000,   0.0000,   0.0000,\n",
       "           0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "           0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "           0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "           0.0000,   0.0000],\n",
       "        [  0.0000,   0.0000,   0.0000,   0.0000,  18.2728,   0.0000,   0.0000,\n",
       "           0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "           0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "           0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "           0.0000,   0.0000],\n",
       "        [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000,  15.6532,   0.0000,\n",
       "           0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "           0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "           0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "           0.0000,   0.0000],\n",
       "        [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,  14.2968,\n",
       "           0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "           0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "           0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "           0.0000,   0.0000],\n",
       "        [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "          11.7051,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "           0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "           0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "           0.0000,   0.0000],\n",
       "        [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "           0.0000,  10.6652,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "           0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "           0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "           0.0000,   0.0000],\n",
       "        [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "           0.0000,   0.0000,  10.1267,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "           0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "           0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "           0.0000,   0.0000],\n",
       "        [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "           0.0000,   0.0000,   0.0000,  10.2749,   0.0000,   0.0000,   0.0000,\n",
       "           0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "           0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "           0.0000,   0.0000],\n",
       "        [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "           0.0000,   0.0000,   0.0000,   0.0000,   9.6197,   0.0000,   0.0000,\n",
       "           0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "           0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "           0.0000,   0.0000],\n",
       "        [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "           0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   9.0057,   0.0000,\n",
       "           0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "           0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "           0.0000,   0.0000],\n",
       "        [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "           0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   8.8082,\n",
       "           0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "           0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "           0.0000,   0.0000],\n",
       "        [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "           0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "           6.8548,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "           0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "           0.0000,   0.0000],\n",
       "        [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "           0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "           0.0000,   6.5595,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "           0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "           0.0000,   0.0000],\n",
       "        [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "           0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "           0.0000,   0.0000,   5.7828,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "           0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "           0.0000,   0.0000],\n",
       "        [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "           0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "           0.0000,   0.0000,   0.0000,   5.5144,   0.0000,   0.0000,   0.0000,\n",
       "           0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "           0.0000,   0.0000],\n",
       "        [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "           0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "           0.0000,   0.0000,   0.0000,   0.0000,   5.0744,   0.0000,   0.0000,\n",
       "           0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "           0.0000,   0.0000],\n",
       "        [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "           0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "           0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   4.7052,   0.0000,\n",
       "           0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "           0.0000,   0.0000],\n",
       "        [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "           0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "           0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   4.3494,\n",
       "           0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "           0.0000,   0.0000],\n",
       "        [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "           0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "           0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "           3.6874,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "           0.0000,   0.0000],\n",
       "        [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "           0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "           0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "           0.0000,   3.3582,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "           0.0000,   0.0000],\n",
       "        [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "           0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "           0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "           0.0000,   0.0000,   3.1852,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "           0.0000,   0.0000],\n",
       "        [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "           0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "           0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "           0.0000,   0.0000,   0.0000,   2.6906,   0.0000,   0.0000,   0.0000,\n",
       "           0.0000,   0.0000],\n",
       "        [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "           0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "           0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "           0.0000,   0.0000,   0.0000,   0.0000,   2.6753,   0.0000,   0.0000,\n",
       "           0.0000,   0.0000],\n",
       "        [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "           0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "           0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "           0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   2.4322,   0.0000,\n",
       "           0.0000,   0.0000],\n",
       "        [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "           0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "           0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "           0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   2.1855,\n",
       "           0.0000,   0.0000],\n",
       "        [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "           0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "           0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "           0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "           1.9222,   0.0000],\n",
       "        [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "           0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "           0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "           0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "           0.0000,   1.6447]], grad_fn=<SubBackward0>)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.diag_embed(S_batch2[-1])[:k, :k] - torch.diag_embed(s)[:k, :k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 784])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(8.6394e-08, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print((B4-B2).abs().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([60])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(8.6394e-08, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=5\n",
    "b=3\n",
    "del a,b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([30, 20, 784, 784])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calculate_B_alter(model, train_z_loader, k, batch_size, first_time = False):\n",
    "    grad_output=torch.ones(batch_size).cuda()\n",
    "    B=[]\n",
    "    for step, (imgs, _) in enumerate(test_loader):\n",
    "        imgs = imgs.view(batch_size, -1).cuda()\n",
    "        imgs.requires_grad_(True)\n",
    "        recover, code_data= model(imgs)\n",
    "    \n",
    "        z = z.view(batch_size, -1).cuda()\n",
    "        z.requires_grad_(True)\n",
    "        recover, code_data= model(z)\n",
    "        dgdx_z=[]   \n",
    "        for i in range(recover.shape[1]):\n",
    "            dgdx_z.append(torch.autograd.grad(outputs=recover[:,i], inputs=z, grad_outputs=grad_output, retain_graph=True)[0])\n",
    "        dgdx_z = torch.reshape(torch.cat(dgdx_z,1), [batch_size, recover.shape[1], z.shape[1]])\n",
    "\n",
    "        u, sigma, v = torch.svd(dgdx_z)\n",
    "        u = u[:, :, :k]\n",
    "        sigma = torch.diag_embed(sigma)[:, :k, :k]\n",
    "        v = torch.transpose(v[:, :, :k],1,2)\n",
    "\n",
    "        b = torch.matmul(u, torch.matmul(sigma, v))\n",
    "        B.append(b.cpu())\n",
    "\n",
    "    B = torch.stack(B)\n",
    "    del dgdx_z\n",
    "    del b\n",
    "    return B\n",
    "\n",
    "B = calculate_B_alter(model, train_z_loader, k, batch_size, first_time = True)\n",
    "B.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20000"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1000*20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 784, 784])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B_iter = iter(B)\n",
    "next(B_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.zeros((30,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "shape '[30, -1]' is invalid for input of size 2352",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-bc67c899e80c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'timeit'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'-n 5'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'U=[]\\nfor step, (imgs, _) in enumerate(test_loader):\\n    imgs = imgs.view(batch_size, -1).cuda()\\n    imgs.requires_grad_(True)\\n    recover, code_data= model(imgs)\\n    grad_output=torch.ones(recover.size()).cuda()\\n    dgdx=[]                                                                                        \\n    #print(torch.autograd.grad(outputs=recover, inputs=imgs, grad_outputs=grad_output, retain_graph=True)[0].shape)\\n    Jac = torch.autograd.functional.jacobian(lambda x: model(x)[0], imgs, create_graph=False, strict=True)\\n    Jac = torch.diagonal(Jac, dim1=0, dim2=2).permute(2, 0, 1)\\n#     print(Jac.size)\\n#     Jx=torch.reshape(torch.cat(Jx,1),[batch_size, code_data.shape[1], imgs.shape[1]])\\n#     print(\"Jx\", Jx.shape)\\n#     u1, sigma, v = torch.svd(Jx)\\n#     print(\\'u\\',u1.shape)\\n#     print(\\'sigma\\',sigma.shape)\\n#     print(\\'v\\',v.shape)\\n    break\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/envs/jupyterhub/lib/python3.8/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2380\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2381\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2382\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2383\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2384\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<decorator-gen-53>\u001b[0m in \u001b[0;36mtimeit\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/jupyterhub/lib/python3.8/site-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/jupyterhub/lib/python3.8/site-packages/IPython/core/magics/execution.py\u001b[0m in \u001b[0;36mtimeit\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[1;32m   1171\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1173\u001b[0;31m         \u001b[0mall_runs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtimer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumber\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1174\u001b[0m         \u001b[0mbest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_runs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnumber\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1175\u001b[0m         \u001b[0mworst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_runs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnumber\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/jupyterhub/lib/python3.8/timeit.py\u001b[0m in \u001b[0;36mrepeat\u001b[0;34m(self, repeat, number)\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m             \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumber\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    206\u001b[0m             \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/jupyterhub/lib/python3.8/site-packages/IPython/core/magics/execution.py\u001b[0m in \u001b[0;36mtimeit\u001b[0;34m(self, number)\u001b[0m\n\u001b[1;32m    167\u001b[0m         \u001b[0mgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m             \u001b[0mtiming\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mgcold\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<magic-timeit>\u001b[0m in \u001b[0;36minner\u001b[0;34m(_it, _timer)\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[30, -1]' is invalid for input of size 2352"
     ]
    }
   ],
   "source": [
    "%%timeit -n 5\n",
    "U=[]\n",
    "for step, (imgs, _) in enumerate(test_loader):\n",
    "    imgs = imgs.view(batch_size, -1).cuda()\n",
    "    imgs.requires_grad_(True)\n",
    "    recover, code_data= model(imgs)\n",
    "    grad_output=torch.ones(recover.size()).cuda()\n",
    "    dgdx=[]                                                                                        \n",
    "    #print(torch.autograd.grad(outputs=recover, inputs=imgs, grad_outputs=grad_output, retain_graph=True)[0].shape)\n",
    "    Jac = torch.autograd.functional.jacobian(lambda x: model(x)[0], imgs, create_graph=False, strict=True)\n",
    "    Jac = torch.diagonal(Jac, dim1=0, dim2=2).permute(2, 0, 1)\n",
    "#     print(Jac.size)\n",
    "#     Jx=torch.reshape(torch.cat(Jx,1),[batch_size, code_data.shape[1], imgs.shape[1]])\n",
    "#     print(\"Jx\", Jx.shape)\n",
    "#     u1, sigma, v = torch.svd(Jx)\n",
    "#     print('u',u1.shape)\n",
    "#     print('sigma',sigma.shape)\n",
    "#     print('v',v.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([30, 784, 784])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "Jac1.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([30, 784, 784])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "U=[]\n",
    "for step, (imgs, _) in enumerate(test_loader):\n",
    "    imgs = imgs.view(batch_size, -1).cuda()\n",
    "    imgs.requires_grad_(True)\n",
    "    recover, code_data= model(imgs)\n",
    "    grad_output=torch.ones(batch_size).cuda()\n",
    "    dgdx=[]   \n",
    "    for i in range(recover.shape[1]):\n",
    "        dgdx.append(torch.autograd.grad(outputs=recover[:,i], inputs=imgs, grad_outputs=grad_output, retain_graph=True)[0])\n",
    "    dgdx=torch.reshape(torch.cat(dgdx,1),[batch_size, recover.shape[1], imgs.shape[1]])\n",
    "    print(dgdx.shape)\n",
    "#     print(\"dgdx\", dgdx.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "u (30, 784, 784)\n",
      "s (30, 784)\n",
      "v (30, 784, 784)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "uu,ss,vv = np.linalg.svd(dgdx.cpu().numpy())\n",
    "print(\"u\",uu.shape)\n",
    "print('s',ss.shape)\n",
    "print('v',vv.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "k=40\n",
    "u, sigma, v = torch.svd(dgdx)\n",
    "u = u[:, :, :k]\n",
    "sigma = torch.diag_embed(sigma)[:, :k, :k]\n",
    "v = torch.transpose(v[:, :, :k],1,2)\n",
    "\n",
    "b = torch.matmul(u, torch.matmul(sigma, v)).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[7.0908e+01, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00],\n",
       "         [0.0000e+00, 5.8376e+01, 0.0000e+00,  ..., 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00],\n",
       "         [0.0000e+00, 0.0000e+00, 5.6948e+01,  ..., 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00],\n",
       "         ...,\n",
       "         [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.1164e-06,\n",
       "          0.0000e+00, 0.0000e+00],\n",
       "         [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
       "          5.6298e-07, 0.0000e+00],\n",
       "         [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
       "          0.0000e+00, 9.7265e-08]],\n",
       "\n",
       "        [[4.2497e+01, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00],\n",
       "         [0.0000e+00, 3.8293e+01, 0.0000e+00,  ..., 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00],\n",
       "         [0.0000e+00, 0.0000e+00, 3.3691e+01,  ..., 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00],\n",
       "         ...,\n",
       "         [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 6.4448e-07,\n",
       "          0.0000e+00, 0.0000e+00],\n",
       "         [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
       "          3.7891e-07, 0.0000e+00],\n",
       "         [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
       "          0.0000e+00, 6.0542e-08]],\n",
       "\n",
       "        [[1.0584e+02, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00],\n",
       "         [0.0000e+00, 9.3129e+01, 0.0000e+00,  ..., 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00],\n",
       "         [0.0000e+00, 0.0000e+00, 7.6485e+01,  ..., 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00],\n",
       "         ...,\n",
       "         [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.3279e-06,\n",
       "          0.0000e+00, 0.0000e+00],\n",
       "         [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
       "          1.2815e-06, 0.0000e+00],\n",
       "         [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
       "          0.0000e+00, 2.3816e-07]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[6.0989e+01, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00],\n",
       "         [0.0000e+00, 5.9479e+01, 0.0000e+00,  ..., 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00],\n",
       "         [0.0000e+00, 0.0000e+00, 5.3173e+01,  ..., 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00],\n",
       "         ...,\n",
       "         [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 7.4029e-07,\n",
       "          0.0000e+00, 0.0000e+00],\n",
       "         [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
       "          4.1329e-07, 0.0000e+00],\n",
       "         [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
       "          0.0000e+00, 7.0362e-08]],\n",
       "\n",
       "        [[5.4274e+01, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00],\n",
       "         [0.0000e+00, 4.3798e+01, 0.0000e+00,  ..., 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00],\n",
       "         [0.0000e+00, 0.0000e+00, 4.0782e+01,  ..., 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00],\n",
       "         ...,\n",
       "         [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.3089e-06,\n",
       "          0.0000e+00, 0.0000e+00],\n",
       "         [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
       "          7.2784e-07, 0.0000e+00],\n",
       "         [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
       "          0.0000e+00, 1.2873e-07]],\n",
       "\n",
       "        [[7.5122e+01, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00],\n",
       "         [0.0000e+00, 6.5448e+01, 0.0000e+00,  ..., 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00],\n",
       "         [0.0000e+00, 0.0000e+00, 5.6210e+01,  ..., 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00],\n",
       "         ...,\n",
       "         [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.3640e-06,\n",
       "          0.0000e+00, 0.0000e+00],\n",
       "         [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
       "          8.2393e-07, 0.0000e+00],\n",
       "         [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
       "          0.0000e+00, 2.6283e-07]]], device='cuda:0')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.diagonal(sigma).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[-1.49011612e-08,  4.65661287e-09,  3.72529030e-08, ...,\n",
       "          0.00000000e+00,  1.11758709e-08,  2.23517418e-08],\n",
       "        [ 2.94297934e-07, -2.81492248e-07,  1.41561031e-07, ...,\n",
       "          2.79396772e-08,  1.32247806e-07,  4.47034836e-08],\n",
       "        [-9.56331939e-02,  1.15927666e-01, -4.90063503e-02, ...,\n",
       "         -9.00560617e-03, -4.14587781e-02, -3.27650905e-02],\n",
       "        ...,\n",
       "        [ 0.00000000e+00,  7.96035603e-02,  4.41132002e-02, ...,\n",
       "          1.18063048e-01,  1.03665143e-03, -1.06260933e-01],\n",
       "        [ 0.00000000e+00, -7.56786913e-02, -5.79742491e-02, ...,\n",
       "          8.13618898e-02, -9.88192409e-02,  5.76680303e-02],\n",
       "        [ 0.00000000e+00, -3.03407423e-02, -3.83895561e-02, ...,\n",
       "          5.59838787e-02,  8.91866069e-03, -1.08216830e-01]],\n",
       "\n",
       "       [[-3.35276127e-08, -2.29105353e-07, -1.93715096e-07, ...,\n",
       "         -4.47034836e-08,  1.49011612e-08,  2.51457095e-08],\n",
       "        [ 1.49011612e-08, -1.67638063e-07,  2.53319740e-07, ...,\n",
       "          2.60770321e-08,  4.84287739e-08,  7.45058060e-09],\n",
       "        [-2.00234354e-08,  1.49011612e-08,  3.21306288e-08, ...,\n",
       "          0.00000000e+00, -7.45058060e-09, -7.45058060e-09],\n",
       "        ...,\n",
       "        [ 0.00000000e+00, -1.02112852e-01, -2.45688595e-02, ...,\n",
       "         -6.32360578e-02,  6.59454465e-02,  1.13216676e-02],\n",
       "        [ 0.00000000e+00, -9.84916240e-02,  1.43916905e-01, ...,\n",
       "         -2.72205342e-02, -5.39901517e-02,  2.99524162e-02],\n",
       "        [ 0.00000000e+00, -5.15707210e-02,  4.69148718e-02, ...,\n",
       "          2.12499909e-02,  1.84704196e-02,  8.48987047e-03]],\n",
       "\n",
       "       [[-1.28056854e-08, -6.14672899e-08, -8.19563866e-08, ...,\n",
       "         -7.45058060e-09,  5.58793545e-09, -2.23517418e-08],\n",
       "        [-7.45058060e-09,  1.76019967e-07,  1.89989805e-07, ...,\n",
       "         -3.72529030e-09,  4.09781933e-08,  3.25962901e-08],\n",
       "        [ 5.77419996e-08,  4.28408384e-08, -9.12696123e-08, ...,\n",
       "          1.22381607e-08, -2.87545845e-08, -3.72529030e-09],\n",
       "        ...,\n",
       "        [ 0.00000000e+00,  4.31450307e-02, -4.03513685e-02, ...,\n",
       "          2.09190361e-02,  2.58018300e-02, -2.70741917e-02],\n",
       "        [ 0.00000000e+00, -8.21346045e-03, -5.26345707e-02, ...,\n",
       "          2.35338248e-02, -1.94401890e-02,  3.93868573e-02],\n",
       "        [ 0.00000000e+00, -1.16640143e-01,  4.66400832e-02, ...,\n",
       "         -4.94175106e-02,  8.28430243e-03,  8.65413249e-03]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[-2.15557572e-02,  6.27513230e-02,  6.00738898e-02, ...,\n",
       "          2.95793358e-02,  3.98573093e-03, -1.06651813e-01],\n",
       "        [ 4.50138375e-02, -1.03812471e-01, -7.47925118e-02, ...,\n",
       "         -3.28090526e-02,  1.18549064e-01,  3.56472470e-02],\n",
       "        [-6.22548573e-02, -1.17711034e-02, -1.01739511e-01, ...,\n",
       "         -7.86924735e-03,  4.45109233e-02,  3.64757283e-03],\n",
       "        ...,\n",
       "        [ 0.00000000e+00, -3.55063602e-02,  1.80364251e-02, ...,\n",
       "          1.16606340e-01, -5.64613938e-02, -9.01358016e-03],\n",
       "        [ 0.00000000e+00, -3.74888778e-02,  7.04760179e-02, ...,\n",
       "         -8.01215842e-02, -5.10664284e-03, -4.52907756e-02],\n",
       "        [ 0.00000000e+00, -4.34975214e-02,  7.67962858e-02, ...,\n",
       "         -5.62445298e-02,  3.30984294e-02,  5.86803677e-03]],\n",
       "\n",
       "       [[-3.72529030e-08, -3.72529030e-08, -1.41561031e-07, ...,\n",
       "          7.45058060e-09, -1.11758709e-08,  1.86264515e-08],\n",
       "        [-4.47034836e-08,  5.58793545e-09, -2.04890966e-08, ...,\n",
       "          3.35276127e-08,  0.00000000e+00,  3.72529030e-09],\n",
       "        [ 8.58203545e-02,  9.77513790e-02, -1.19271159e-01, ...,\n",
       "         -7.92726874e-02,  6.24129660e-02, -8.59977975e-02],\n",
       "        ...,\n",
       "        [ 0.00000000e+00, -1.12331502e-01, -2.94368677e-02, ...,\n",
       "         -3.00586447e-02, -6.31037131e-02,  5.84929138e-02],\n",
       "        [ 0.00000000e+00, -3.10432073e-02, -3.08895893e-02, ...,\n",
       "          2.11068131e-02,  3.16915214e-02,  1.25477284e-01],\n",
       "        [ 0.00000000e+00,  8.28439966e-02,  4.31986898e-02, ...,\n",
       "         -7.79343098e-02,  8.29579383e-02, -2.92381011e-02]],\n",
       "\n",
       "       [[-3.35276127e-08,  8.10250640e-08,  1.80676579e-07, ...,\n",
       "          5.58793545e-09, -3.84170562e-09, -6.51925802e-09],\n",
       "        [-7.45058060e-08,  5.86733222e-08,  2.32830644e-09, ...,\n",
       "          2.60770321e-08,  2.04890966e-08,  1.49011612e-08],\n",
       "        [ 1.84021682e-01,  8.35977122e-03, -1.57241315e-01, ...,\n",
       "          3.46536413e-02, -3.88627569e-03,  9.38390265e-04],\n",
       "        ...,\n",
       "        [ 0.00000000e+00, -4.91251126e-02,  5.43283187e-02, ...,\n",
       "         -7.23841414e-02,  3.05486880e-02,  3.78315002e-02],\n",
       "        [ 0.00000000e+00, -1.59866214e-01, -1.83140449e-02, ...,\n",
       "         -4.03030701e-02, -2.48217396e-02,  3.22095901e-02],\n",
       "        [ 0.00000000e+00,  2.29242519e-02, -3.68841887e-02, ...,\n",
       "          1.08069414e-02,  5.45447841e-02, -5.96818561e-03]]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.transpose(v,1,2).cpu().numpy() - vv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "u torch.Size([30, 784, 784])\n",
      "sigma torch.Size([30, 784])\n",
      "v torch.Size([30, 784, 784])\n"
     ]
    }
   ],
   "source": [
    "u, sigma, v = torch.svd(dgdx, some=False)\n",
    "print('u',u.shape)\n",
    "print('sigma',sigma.shape)\n",
    "print('v',v.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
